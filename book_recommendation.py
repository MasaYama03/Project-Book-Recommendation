# -*- coding: utf-8 -*-
"""Book_Recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XljpOZjgGQi1kEenessdr2Qt-Iu-VTbz

# Project : Book Recommendation dengan menggunakan Metode Collaborative Filtering

Nama: : Masahiro Gerarudo Yamazaki<br>
Email: : masahiroymzk24@gmail.com<br>
ID Dicoding: : masayama<br>

# Import Library
"""

!pip install optuna

"""Pengimporan pustaka yang relevan untuk mendukung pemrosesan data hingga pengembangan model."""

from google.colab import files
import os
import zipfile
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
import optuna

"""### Menyiapkan Kredensial Kaggle

Dalam proyek ini, dataset diperoleh dari platform Kaggle. Untuk dapat mengunduh dataset secara langsung melalui API Kaggle, diperlukan kredensial berupa file JSON yang dapat diunduh dari halaman profil akun Kaggle (menu: My Account > Your Profile > Settings > Account > API > Create New API Token). File tersebut kemudian diunggah ke lingkungan kerja sebelum proses pengunduhan dapat dilakukan.

Selain metode pengunduhan langsung dari Kaggle menggunakan API, terdapat beberapa alternatif untuk memuat dataset, antara lain:

1. Mengunduh manual melalui situs Kaggle, lalu mengunggah file secara manual ke lingkungan kerja seperti Google Colab atau Jupyter Notebook.

2. Menggunakan Google Drive, dengan menyimpan dataset hasil unduhan ke Google Drive dan mengaksesnya melalui integrasi Google Drive di Colab.

3. Memanfaatkan cloud storage lain seperti Dropbox atau AWS S3, jika dataset disimpan di sana.

4. Menggunakan dataset lokal, apabila proyek dijalankan di mesin lokal dan file dataset sudah tersedia di direktori lokal.

Pemilihan metode tergantung pada preferensi pengguna serta keterbatasan platform yang digunakan dalam eksperimen.
"""

# Upload kaggle.json

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}"'.format(
      name=fn))

# Ubah permission file
!chmod 600 /content/kaggle.json

# Setup Kaggle environment
os.environ['KAGGLE_CONFIG_DIR'] = "/content"

"""### Mengunduh Dataset

Sumber dataset : [Book Recommendation Dataset]('https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset')

setelah melakukan import library dan menyiapkan API Token pada kaggle yang dibutuhkan untuk project ini, kita sudah bisa melakukan load data dari kaggle

Informasi Terkait Dataset:

Jenis | Keterangan
--- | ---
Title | Book Recommendation Dataset
Source | [Kaggle](https://www.kaggle.com/arashnic/book-recommendation-dataset)
Maintainer | [MÃ¶bius](https://www.kaggle.com/arashnic)
License | [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)
Usability | 10.0
"""

# Download Dataset
!kaggle datasets download -d arashnic/book-recommendation-dataset

# melakukan ekstraksi pada file zip
local_zip = 'book-recommendation-dataset.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content/book-recommendation-dataset/')
zip_ref.close()

# Setelah diekstraksi maka kita bisa langsung menghapus berkas zip yang sudah tidak diperlukan
!rm book-recommendation-dataset.zip

"""## Data Understanding

Pada Dataset ini terdapat 3 berkas csv diantaranya yaitu Books.csv , Ratings.csv , dan Users.csv

Selanjutnya kita akan melihat informasi dataset dengan bantuan `pandas` dimana ini membantu untuk melihat isi dari masing-masing berkas csv tersebut.
"""

# Load dataset

books = pd.read_csv('book-recommendation-dataset/Books.csv')
ratings = pd.read_csv('book-recommendation-dataset/Ratings.csv')
users = pd.read_csv('book-recommendation-dataset/Users.csv')

"""### Books

Berikut ini adalah isi dari `Books.csv`
"""

books.head(6)

books.info()

"""Dari keluaran di atas dapat diketahui bahwa berkas `Books.csv` memuat data-data buku yang terdiri dari 271360 baris dan memiliki 8 kolom, diantaranya adalah :  

- `ISBN` : berisi kode ISBN dari buku  
- `Book-Title` : berisi judul buku
- `Book-Author` : berisi penulis buku
- `Year-Of-Publication` : tahun terbit buku  
- `Publisher` : penerbit buku  
- `Image-URL-S` : URL menuju gambar buku berukuran kecil
- `Image-URL-M` : URL menuju gambar buku berukuran sedang
- `Image-URL-L` : URL menuju gambar buku berukuran besar

### Ratings

Berikut ini adalah isi dari berkas `Ratings.csv`
"""

ratings

ratings.groupby('Book-Rating').count()

rating_counter = ratings.groupby('Book-Rating').count()
plt.figure(figsize=(10,5))
plt.title('Jumlah Rating Buku yang Diberikan Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah Buku')
plt.bar(rating_counter.index, rating_counter['ISBN'])
plt.grid(True)
plt.show()

"""Berdasarkan visualisasi data di atas, terlihat bahwa distribusi data tidak merata dan terdapat banyak pengguna yang memberikan rating dengan nilai 0."""

ratings.info()

ratings.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))

"""Dari keluaran di atas dapat diketahui bahwa berkas `Ratings.csv` memuat data rating buku yang diberikan oleh pengguna. Data ini memiliki 1149780 baris dan memiliki 3 kolom, yaitu :  
 - `User-ID` : berisi ID unik pengguna
 - `ISBN` : berisi kode ISBN buku yang diberi rating oleh pengguna
 - `Book-Rating` : berisi nilai rating yang diberikan oleh pengguna berkisar antara 0-10

### Users

Berikut ini adalah isi dari `Users.csv`
"""

users.head(6)

users.info()

users.describe()

"""Dari keluaran di atas dapat diketahui bahwa berkas `Users.csv` memuat data pengguna. Data ini terdiri dari 278858 baris dan memiliki 3 kolom, yaitu :

- `User-ID` : berisi ID unik pengguna
- `Location` : berisi data lokasi pengguna
- `Age` : berisi data usia pengguna

## Data Preparation

Sebelum dapat ke tahap permodelan, maka data harus melalui tahap data preparation terlebih dahulu karena "Garbage In, Garbage Out" (GIGO) yang artinya kualitas output model sangat tergantung pada kualitas input datanya.

Berikut adalah langkah-langkah yang perlu dilakukan dalam data preparation.

### Handling Imbalanced Data

Karena sebelumnya ditemukan bahwa distribusi rating tidak seimbang, pada tahap ini saya melakukan upaya penanganan dengan menghapus data yang memiliki rating bernilai 0.
"""

ratings.drop(ratings[ratings["Book-Rating"] == 0].index, inplace=True)

"""Berikut ini adalah jumlah data setelah di-drop"""

ratings.shape

ratings.head(6)

rating_counter = ratings.groupby('Book-Rating').count()
plt.figure(figsize=(10,5))
plt.title('Jumlah Rating Buku yang Diberikan Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah Buku')
plt.bar(rating_counter.index, rating_counter['ISBN'])
plt.grid(True)
plt.show()

"""### Encoding Data

Encoding dilakukan untuk menyandikan `User-ID` dan `ISBN` ke dalam indeks integer
"""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = ratings['User-ID'].unique().tolist()

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Mengubah ISBN menjadi list tanpa nilai yang sama
isbn_list = ratings['ISBN'].unique().tolist()

# Melakukan encoding ISBN
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_list)}

# Melakukan proses encoding angka ke ISBN
isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn_list)}

"""Setelah itu hasil dari encoding akan dimapping ke dataframe `ratings`"""

# Mapping userID ke dataframe user
ratings['user'] = ratings['User-ID'].map(user_to_user_encoded)

# Mapping userID ke dataframe user
ratings['book'] = ratings['ISBN'].map(isbn_to_isbn_encoded)

ratings.head(6)

ratings.info()

"""### Randomize Dataset

Berikut ini adalah proses pengacakan data agar distribusi datanya menjadi random.
"""

# Mengacak dataset
df = ratings.sample(frac=1, random_state=42)
df.head(10)

"""### Data Standardization and Splitting

Setelah proses pengacakan data, dataset kemudian dibagi menjadi dua bagian: 80% digunakan untuk melatih model dan 20% sisanya digunakan untuk proses validasi.

Selain itu, dilakukan juga normalisasi pada nilai rating, yang semula berada dalam rentang 0 hingga 10, diubah menjadi rentang 0 hingga 1 agar proses pelatihan model menjadi lebih efisien dan stabil.
"""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah resto
num_isbn = len(isbn_encoded_to_isbn)
print(num_isbn)

# Mengubah rating menjadi nilai float
df['Book-Rating'] = df['Book-Rating'].values.astype(np.float32)

# Nilai minimum Book-Rating
min_rating = min(df['Book-Rating'])

# Nilai maksimal Book-Rating
max_rating = max(df['Book-Rating'])

print('Number of User: {}, Number of ISBN: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_isbn, min_rating, max_rating
))

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = df[['user', 'book']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""## Modelling
Setelah melalui proses data preparation, maka data sudah siap masuk ke tahap modelling

### Membuat Kelas RecommenderNet
"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_isbn, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_isbn = num_isbn
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings book
        num_isbn,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_isbn, 1) # layer embedding book bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_book = tf.tensordot(user_vector, book_vector, 2)

    x = dot_user_book + user_bias + book_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""### Hyperparameter Tuning

Agar mendapatkan hasil model yang optimal, maka dalam proyek ini menggunakan bantuan library `optuna` untuk melakukan hyperparameter tuning atau pencarian nilai hyperparameter yang terbaik, dalam hal ini adalah nilai `embedding_size`.
"""

def objective(trial):
    tf.keras.backend.clear_session()
    model = RecommenderNet(num_users=num_users, num_isbn=num_isbn, embedding_size=trial.suggest_int('embedding_size', 1, 15))

    # model compile
    model.compile(
        loss = tf.keras.losses.BinaryCrossentropy(),
        optimizer = keras.optimizers.Adam(learning_rate=0.001),
        metrics=[tf.keras.metrics.RootMeanSquaredError()]
    )

    model.fit(
        x = x_train,
        y = y_train,
        batch_size=200,
        epochs = 1,
        validation_data = (x_val, y_val)
    )

    y_pred= model.predict(x_val)

    return np.sqrt(mean_squared_error(y_val, y_pred))

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=15, timeout=500)

print("Number of finished trials: {}".format(len(study.trials)))

print("Best trial:")
trial = study.best_trial

print("  Value: {}".format(trial.value))

print("  Params: ")
for key, value in trial.params.items():
    print("    {}: {}".format(key, value))

tf.keras.backend.clear_session()

# Menerapkan nilai parameter paling optimal dari optuna
BEST_EMBEDDING_SIZE = 1

model = RecommenderNet(num_users, num_isbn, BEST_EMBEDDING_SIZE)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""### Melatih Model"""

# Callback untuk menghentikan pelatihan lebih awal jika tidak ada peningkatan pada val_rmse
early_stop = EarlyStopping(
    monitor='val_root_mean_squared_error',  # Metrik yang dipantau
    patience=3,                              # Berapa epoch menunggu sebelum berhenti
    restore_best_weights=True,               # Mengembalikan bobot terbaik
    mode='min'                               # Karena kita ingin nilai RMSE serendah mungkin
)

# Memulai training
history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=64,
    epochs=10,
    validation_data=(x_val, y_val),
    callbacks=[early_stop]
)

"""## Evaluasi"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.grid(True)
plt.show()

"""Berdasarkan metrik evaluasi, model yang dikembangkan menghasilkan nilai Root Mean Squared Error (RMSE) sebesar 0.184

## Mendapatkan Rekomendasi
"""

books_df = books
df = pd.read_csv('book-recommendation-dataset/Ratings.csv')

# Mengambil sample user
user_id = df['User-ID'].sample(1).iloc[0]
book_read_by_user = df[df['User-ID'] == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
book_not_read = books_df[~books_df['ISBN'].isin(book_read_by_user.ISBN.values)]['ISBN']
book_not_read = list(
    set(book_not_read)
    .intersection(set(isbn_to_isbn_encoded.keys()))
)

book_not_read = [[isbn_to_isbn_encoded.get(x)] for x in book_not_read]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_read), book_not_read)
)

ratings = model.predict(user_book_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_isbns = [
    isbn_encoded_to_isbn.get(book_not_read[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Books with high ratings from user')
print('----' * 8)

top_book_user = (
    book_read_by_user.sort_values(
        by = 'Book-Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

book_df_rows = books_df[books_df['ISBN'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row._3, "-", row._2)

print('----' * 8)
print('Top 10 book recommendation')
print('----' * 8)

recommended_books = books_df[books_df['ISBN'].isin(recommended_book_isbns)]
for row in recommended_books.itertuples():
    print(row._3, "-", row._2)

"""## Modeling telah selesai, Selanjutnya bisa dilakukan save model apabila ingin di kembangkan ke bentuk aplikasi"""